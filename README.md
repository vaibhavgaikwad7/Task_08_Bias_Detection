# Task 08: Bias Detection in LLM Data Narratives

This repository contains the complete implementation of Research Task 08, which examines how Large Language Models (LLMs) may produce biased narratives when interpreting identical data under different prompt framings.  
The project explores narrative bias in the context of anonymized basketball statistics (Players Aâ€“E) using systematic and reproducible methods.

All work included here follows Syracuse University OPT research guidelines, including dataset anonymization, reproducible analysis, and separation of raw model outputs.

---

## ğŸ“˜ Project Overview

LLMs can generate different interpretations of the same data depending on how a question is framed. In this project, I evaluate four types of potential bias:

- **Confirmation Bias**  
- **Negative Framing Bias**
- **Positive Framing Bias**
- **Demographic Bias**  

All prompts reference the same dataset, allowing the framing of the question to be the only independent variable.  
Model responses are collected manually and stored locally to comply with OPT and FERPA regulations.

The final report (`REPORT.md`) summarizes the findings, methods, and conclusions from the bias evaluation.

---

## ğŸ“‚ Repository Structure

Task_08_Bias_Detection/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ anonymized_stats.csv         # Only dataset allowed in repo (PII removed)
â”‚
â”œâ”€â”€ prompts/                         # Prompt templates for each bias condition
â”‚   â”œâ”€â”€ confirmation_bias.txt
â”‚   â”œâ”€â”€ framing_positive.txt
â”‚   â”œâ”€â”€ framing_negative.txt
â”‚   â””â”€â”€ demographic_bias.txt
â”‚
â”œâ”€â”€ experiments/                     # Python scripts for running experiments
â”‚   â”œâ”€â”€ experiment_design.py         # Generates prompt JSON from dataset
â”‚   â”œâ”€â”€ run_experiment.py            # Helps log/structure LLM responses
â”‚   â”œâ”€â”€ analyze_bias.py              # Creates tables, plots, bias metrics
â”‚   â””â”€â”€ validate_claims.py           # Checks numerical accuracy of LLM outputs
â”‚
â”œâ”€â”€ results/                         # Real LLM outputs stored locally only
â”‚   â””â”€â”€ .keep                        # (folder tracked, contents ignored)
â”‚
â”œâ”€â”€ analysis/                        # Auto-generated analysis outputs (tables/graphs)
â”‚
â”œâ”€â”€ REPORT.md                        # Full written report for Task 08
â””â”€â”€ README.md                        # You are here

---

## ğŸ› ï¸ How to Run the Pipeline

Before running the scripts, install dependencies:

pip install -r requirements.txt

Then run the pipeline in order:

### **1ï¸âƒ£ Create Prompt Variations**
Generates all prompts in JSON format based on the dataset.

python experiments/experiment_design.py

### **2ï¸âƒ£ Collect LLM Responses**
This script provides structure for logging responses.  
You will manually copy and paste LLM outputs (Gemini, ChatGPT, Claude, etc.) into:

results/experiment_log.json

python experiments/run_experiment.py

### **3ï¸âƒ£ Run Bias + Sentiment Analysis**
Produces CSV tables and graphs inside the analysis/ folder.

python experiments/analyze_bias.py

### **4ï¸âƒ£ Validate Numerical Claims**
Checks whether LLM outputs match the ground-truth dataset.

python experiments/validate_claims.py

---

## ğŸ§ª Experiment Design Summary

- **Data:** Five anonymized players (Aâ€“E)
- **Metrics:** Points, Rebounds, Assists, Steals
- **Bias Types Tested:** Confirmation, Positive/Negative Framing, Demographic
- **Independent Variable:** Prompt phrasing only
- **Dependent Variable:** Narrative content of LLM output
- **Models Tested:** Gemini (others optional)

All prompts reference the same dataset to isolate narrative drift caused by framing.

---

## ğŸ“„ Final Report

The full analysisâ€”including hypotheses, results, bias catalogue, mitigation strategies, and limitationsâ€”is found in:

REPORT.md

This document serves as the primary deliverable for Research Task 08.

---

## ğŸ”’ Compliance Notes

- No raw or identifiable datasets are included.  
- All player names are replaced with anonymous identifiers.  
- Only anonymized_stats.csv is committed.  
- The results/ folder is intentionally excluded via .gitignore.  
- The repository is fully reproducible and clean of PII.

---
